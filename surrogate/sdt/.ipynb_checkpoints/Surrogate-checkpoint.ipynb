{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "from PIL import Image\n",
    "import argparse\n",
    "\n",
    "from pl_bolts.models.self_supervised import SimCLR\n",
    "from pl_bolts.models.self_supervised.simclr import SimCLREvalDataTransform, SimCLRTrainDataTransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import load_data\n",
    "from main import build_dt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SimCLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input[0]\n",
    "\n",
    "weight_path = 'https://pl-bolts-weights.s3.us-east-2.amazonaws.com/simclr/bolts_simclr_imagenet/simclr_imagenet.ckpt'\n",
    "simclr = SimCLR.load_from_checkpoint(weight_path, strict=False)\n",
    "model = nn.Sequential(\n",
    "    simclr.encoder,\n",
    "    Flatten(),\n",
    "    nn.Linear(2048,2))\n",
    "model.load_state_dict(torch.load('resnet50_simclr_crop_12', map_location='cpu'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Swav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pl_bolts.models.self_supervised import SwAV\n",
    "\n",
    "weight_path = 'https://pl-bolts-weights.s3.us-east-2.amazonaws.com/swav/swav_imagenet/swav_imagenet.pth.tar'\n",
    "model = SwAV.load_from_checkpoint(weight_path, strict=True).model\n",
    "model.prototypes=nn.Linear(128, 2)\n",
    "\n",
    "model.load_state_dict(torch.load('resnet50_swav_crop_10', map_location='cpu'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model accuracy test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "BATCH = 16\n",
    "#image_size = [224,224]\n",
    "image_size = [373,373]\n",
    "train_data = datasets.ImageFolder(root=\"../../data/chest_xray/train\", transform= transforms.Compose([\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "            ])\n",
    "                                 )\n",
    "train_data_loader = DataLoader(train_data, batch_size=BATCH, num_workers=4)\n",
    "test_data = datasets.ImageFolder(root=\"../../data/chest_xray/test\",transform= transforms.Compose([\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "            ]))\n",
    "\n",
    "\n",
    "test_data_loader = DataLoader(test_data, batch_size = BATCH, num_workers=4)\n",
    "\n",
    "#images, labels = next(iter(test_data_loader))\n",
    "#plt.imshow(images[0][0])\n",
    "\n",
    "def test_loop(dataloader, model):\n",
    "    size = len(dataloader.dataset)\n",
    "    print(\"Taille \", size)\n",
    "    num_batches = len(dataloader)\n",
    "    print(\"Batchs \", num_batches)\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            emb, pred = pred\n",
    "            \n",
    "            correct+= (pred.argmax(1)==y).type(torch.float).sum().item()\n",
    "            print(100*correct/size)\n",
    "    correct/= size\n",
    "    return 100*correct\n",
    "test_loop(test_data_loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and Train DT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LOADER_PATH = \"loaders/SimCLR_crop/dataloader.p\"\n",
    "TEST_LOADER_PATH = \"loaders/SimCLR_crop/testloader.p\"\n",
    "CROP = True #True for models based on crop images dataset\n",
    "EMB_OUTPUT = False #True for models outputing the embedding in addition with the prediction\n",
    "TRAIN_DATA_PATH = \"../../data/chest_xray/train\"\n",
    "TEST_DATA_PATH = \"../../data/chest_xray/test\"\n",
    "MODEL_PATH = \"dtmodels/SimCLR_crop/bestmodel2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(DATA_LOADER_PATH):\n",
    "    train_loader = pickle.load(open(DATA_LOADER_PATH, \"rb\"))\n",
    "else:\n",
    "    train_loader = load_data(model, TRAIN_DATA_PATH, crop=CROP, emb_output = EMB_OUTPUT)\n",
    "    pickle.dump(train_loader, open( DATA_LOADER_PATH, \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(TEST_LOADER_PATH):\n",
    "    test_loader = pickle.load(open(TEST_LOADER_PATH, \"rb\"))\n",
    "else:\n",
    "    test_loader = load_data(model, TEST_DATA_PATH, crop=CROP, emb_output = EMB_OUTPUT)\n",
    "    pickle.dump(test_loader, open(TEST_LOADER_PATH, \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description='DT model')\n",
    "parser.add_argument('--batch-size', type=int, default=50, metavar='N',\n",
    "                    help='input batch size for training (default: 64)')\n",
    "parser.add_argument('--input-dim', type=int, default=224 * 224 * 3, metavar='N',\n",
    "                    help='input dimension size(default: 224 * 224 * 3)')\n",
    "parser.add_argument('--output-dim', type=int, default=2, metavar='N',\n",
    "                    help='output dimension size(default: 2)')\n",
    "parser.add_argument('--max-depth', type=int, default=2, metavar='N',\n",
    "                    help='maximum depth of tree(default: 8)')\n",
    "parser.add_argument('--epochs', type=int, default=10, metavar='N',\n",
    "                    help='number of epochs to train (default: 5)')\n",
    "parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n",
    "                    help='learning rate (default: 0.01)')\n",
    "parser.add_argument('--lmbda', type=float, default=0.1, metavar='LR',\n",
    "                    help='temperature rate (default: 0.1)')\n",
    "parser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n",
    "                    help='SGD momentum (default: 0.5)')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='disables CUDA training')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                    help='random seed (default: 1)')\n",
    "parser.add_argument('--log-interval', type=int, default=5, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "args.cuda = False\n",
    "# args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "#torch.manual_seed(args.seed)\n",
    "#if args.cuda:\n",
    "    #torch.cuda.manual_seed(args.seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(MODEL_PATH):\n",
    "    model_tree = pickle.load(open(MODEL_PATH, \"rb\"))\n",
    "else:\n",
    "    model_tree = build_dt(args)\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        model_tree.train_(train_loader, epoch, crop=CROP)\n",
    "    pickle.dump(model_tree, open(MODEL_PATH, \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DT model fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tree.test_(train_loader, crop=CROP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = model_tree.buildTree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.render('dt_viz', view=True)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
